{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\developsoft\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_op=pd.read_csv(r\"D:\\DA_competition\\DC\\data\\operation_train.csv\")\n",
    "train_tag=pd.read_csv(r\"D:\\DA_competition\\DC\\data\\tag_train.csv\")\n",
    "train_tr=pd.read_csv(r\"D:\\DA_competition\\DC\\data\\transaction_train.csv\")\n",
    "\n",
    "#表连接\n",
    "train_op=pd.merge(train_op,train_tag,on=\"UID\",how=\"left\")\n",
    "train_op=train_op.drop_duplicates()\n",
    "train_tr=pd.merge(train_tr,train_tag,on=\"UID\",how=\"left\")\n",
    "train_tr=train_tr.drop_duplicates()\n",
    "\n",
    "test_op=pd.read_csv(r\"D:\\DA_competition\\DC\\data\\operation_test.csv\")\n",
    "test_tr=pd.read_csv(r\"D:\\DA_competition\\DC\\data\\transaction_test.csv\")\n",
    "test_op=test_op.drop_duplicates()\n",
    "test_tr=test_tr.drop_duplicates()\n",
    "\n",
    "#处理时间\n",
    "train_op[\"hour\"]=train_op.time.apply(lambda x:x.split(\":\")[0])\n",
    "train_tr[\"hour\"]=train_tr.time.apply(lambda x:x.split(\":\")[0])\n",
    "test_op[\"hour\"]=test_op.time.apply(lambda x:x.split(\":\")[0])\n",
    "test_tr[\"hour\"]=test_tr.time.apply(lambda x:x.split(\":\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op[\"device_code\"]=train_op[[\"device_code1\",\"device_code2\",\"device_code3\"]].fillna(method=\"bfill\",axis=1).iloc[:,0]\n",
    "train_op[\"ip\"]=train_op[[\"ip1\",\"ip2\"]].fillna(method=\"bfill\",axis=1).iloc[:,0]\n",
    "train_op[\"ip_sub\"]=train_op[[\"ip1_sub\",\"ip2_sub\"]].fillna(method=\"bfill\",axis=1).iloc[:,0]\n",
    "\n",
    "test_op[\"device_code\"]=test_op[[\"device_code1\",\"device_code2\",\"device_code3\"]].fillna(method=\"bfill\",axis=1).iloc[:,0]\n",
    "test_op[\"ip\"]=test_op[[\"ip1\",\"ip2\"]].fillna(method=\"bfill\",axis=1).iloc[:,0]\n",
    "test_op[\"ip_sub\"]=test_op[[\"ip1_sub\",\"ip2_sub\"]].fillna(method=\"bfill\",axis=1).iloc[:,0]\n",
    "\n",
    "train_tr[\"device_code\"]=train_tr[[\"device_code1\",\"device_code2\",\"device_code3\"]].fillna(method=\"bfill\",axis=1).iloc[:,0]\n",
    "test_tr[\"device_code\"]=test_tr[[\"device_code1\",\"device_code2\",\"device_code3\"]].fillna(method=\"bfill\",axis=1).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#operation表\n",
    "#1、UID对应mode,success,os,version:cross_type\n",
    "#transaction表\n",
    "#1、UID对应channel、amt_src1、trans_type1、amt_src2、trans_type2、market_type：cross_type\n",
    "op_fields1=[\"mode\",\"success\",\"os\",\"version\"]\n",
    "tr_fields1=[\"channel\",\"amt_src1\",\"trans_type1\",\"amt_src2\",\"trans_type2\"]\n",
    "def get_discrete_fts1(train_data,test_data,fields):\n",
    "    data_one_hot=pd.get_dummies(pd.concat([train_data[fields].applymap(str),test_data[fields].applymap(str)],axis=0),dummy_na=True)\n",
    "    train_data_one_hot=data_one_hot.iloc[:len(train_data),:].reset_index(drop=True)\n",
    "    test_data_one_hot=data_one_hot.iloc[len(train_data):,:].reset_index(drop=True)\n",
    "    train_data_one_hot[\"UID\"]=train_data[\"UID\"]\n",
    "    test_data_one_hot[\"UID\"]=test_data[\"UID\"]\n",
    "    train_fts_cross_type=train_data_one_hot.groupby(\"UID\").sum().reset_index(drop=False)\n",
    "    test_fts_cross_type=test_data_one_hot.groupby(\"UID\").sum().reset_index(drop=False)\n",
    "    \n",
    "    print(\"=====OK======\")\n",
    "    return train_fts_cross_type,test_fts_cross_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#operation表\n",
    "# 2.1、UID对应day、time、mode、success、os、version、device1、device2、geo_code种类数\n",
    "# 2.2、UID对应device_code1、2、3、mac1、2、wifi、ip1、ip2、ip1_sub、ip2_sub种类数\n",
    "#transaction表\n",
    "# 2.1、UID对应channel,day,time,amt_src1,amt_src2,merchant,code1,code2,trans_type1,trans_type2,device1,device2,geo_code,market_code,market_type种类数\n",
    "# 2.2、UID对应ip1、ip1_sub、acc_id1、acc_id2、acc_id3、device_code1、2、3、mac1数\n",
    "op_fields2=[\"day\",\"hour\",\"mode\",\"success\",\"os\",\"version\",\"device1\",\"device2\",\"device_code\",\"ip\",\"ip_sub\"]\n",
    "tr_fields2=[\"channel\",\"day\",\"hour\",\"amt_src1\",\"amt_src2\",\"merchant\",\"trans_type1\",\"trans_type2\",\"device2\",\"ip1\",\"ip1_sub\",\"acc_id1\",\"mac1\",\"device_code\"]\n",
    "def get_kinds_fts2(data,fields):\n",
    "    fts=data.groupby(\"UID\")[fields].apply(lambda x:x.nunique()).reset_index(drop=False)\n",
    "    print(\"=====OK======\")\n",
    "    return fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#operation表\n",
    "# 3.1、UID的device_code1、2、3、mac1、2、wifi、ip1、ip2、ip1_sub、ip2_sub对应的用户数量均值、最大值、最小值（因为一个UID可能对应多个连续字段）\n",
    "# 3.2、UID的device_code1、2、3、mac1、2、wifi、ip1、ip2、ip1_sub、ip2_sub对应的用户羊毛党概率占比均值、最大值、最小值\n",
    "#transaction表\n",
    "# 3.1、UID对应ip1、ip1_sub、acc_id1、acc_id2、acc_id3、device_code1、2、3、mac1对应的用户数量均值、最大值、最小值（因为一个UID可能对应多个连续字段）\n",
    "# 3.2、UID对应ip1、ip1_sub、acc_id1、acc_id2、acc_id3、device_code1、2、3、mac1对应的用户羊毛党概率占比均值、最大值、最小值\n",
    "op_fields3=[\"mac1\",\"mac2\",\"wifi\",\"device_code\",\"ip\",\"ip_sub\"]\n",
    "tr_fields3=[\"ip1\",\"ip1_sub\",\"acc_id1\",\"acc_id2\",\"acc_id3\",\"mac1\",\"device_code\"]\n",
    "# def get_fields_as_key_fts(data,fields):\n",
    "#     fts_all=pd.DataFrame(data[\"UID\"].unique())\n",
    "#     fts_all.columns=[\"UID\"]\n",
    "#     for field in fields:\n",
    "        \n",
    "#         d=data[[\"UID\",field,\"Tag\"]].drop_duplicates().fillna(0).groupby(field)\n",
    "#         f_count=d.Tag.count()\n",
    "#         count_1=d.Tag.apply(lambda x:len(x[x==1]))\n",
    "        \n",
    "#         df=pd.merge(pd.DataFrame(f_count),pd.DataFrame(count_1),how=\"left\",left_index=True,right_index=True)\n",
    "#         df.columns=[\"user_counts\",\"black_counts\"]\n",
    "#         df=df.reset_index(drop=False)\n",
    "#         df[\"black_ratio\"]=df.black_counts/df.user_counts\n",
    "        \n",
    "#         fts_mid=data[[\"UID\",field]].fillna(0)\n",
    "#         fts_mid=pd.merge(fts_mid,df,how=\"left\",on=field).drop_duplicates()\n",
    "        \n",
    "#         fm_group=fts_mid.groupby(\"UID\")\n",
    "        \n",
    "#         fts=pd.DataFrame(fm_group.user_counts.mean())\n",
    "#         fts.columns=[field+\"count_mean\"]\n",
    "#         fts[field+\"count_max\"]=fm_group.user_counts.max()\n",
    "#         fts[field+\"count_min\"]=fm_group.user_counts.min()\n",
    "#         fts[field+\"ratio_mean\"]=fm_group.black_counts.sum()/fm_group.user_counts.sum()\n",
    "#         fts[field+\"ratio_max\"]=fm_group.black_ratio.max()\n",
    "#         fts[field+\"ratio_min\"]=fm_group.black_ratio.min()\n",
    "#         fts=fts.reset_index(drop=False)\n",
    "#         print(fts)\n",
    "#         fts_all=pd.merge(fts_all,fts,how=\"left\",on=\"UID\")\n",
    "#     return fts_all\n",
    "def get_fields_as_key_fts3(data,fields):\n",
    "    fts_all=pd.DataFrame(data[\"UID\"].unique())\n",
    "    fts_all.columns=[\"UID\"]\n",
    "    for field in fields:\n",
    "        \n",
    "        f_count=data[[\"UID\",field]].drop_duplicates().groupby(field).count()\n",
    "        df=pd.DataFrame(f_count)\n",
    "        df.columns=[\"user_counts\"]\n",
    "        df=df.reset_index(drop=False)\n",
    "        \n",
    "        fts_mid=data[[\"UID\",field]].fillna(0)\n",
    "        fts_mid=pd.merge(fts_mid,df,how=\"left\",on=field).drop_duplicates()\n",
    "        \n",
    "        fm_group=fts_mid.groupby(\"UID\")\n",
    "        \n",
    "        fts=pd.DataFrame(fm_group.user_counts.mean())\n",
    "        fts.columns=[field+\"count_mean\"]\n",
    "        fts[field+\"count_max\"]=fm_group.user_counts.max()\n",
    "        fts[field+\"count_min\"]=fm_group.user_counts.min()\n",
    "        fts=fts.reset_index(drop=False)\n",
    "        fts_all=pd.merge(fts_all,fts,how=\"left\",on=\"UID\")\n",
    "    print(\"=====OK======\")\n",
    "    return fts_all\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# operation表\n",
    "# transaction表\n",
    "# 4、UID对应样本数\n",
    "def get_sample_counts_fts4(data):\n",
    "    fts=pd.DataFrame(data[\"UID\"].value_counts().sort_index()).reset_index(drop=False)\n",
    "    fts.columns=[\"UID\",\"UID_sample_counts\"]\n",
    "    print(\"=====OK======\")\n",
    "    return fts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transaction表\n",
    "# UID对应trans_amt、bal的和、平均值、最大值、最小值\n",
    "tr_fields5=[\"trans_amt\",\"bal\"]\n",
    "def get_tr_money_fts5(data,fields):\n",
    "    group=data.groupby(\"UID\")[fields]\n",
    "    fts1=group.apply(lambda x:x.sum())\n",
    "    fts1.columns=[f+\"_sum\" for f in fields]\n",
    "    fts2=group.apply(lambda x:x.max())\n",
    "    fts2.columns=[f+\"_max\" for f in fields]\n",
    "    fts3=group.apply(lambda x:x.min())\n",
    "    fts3.columns=[f+\"_min\" for f in fields]\n",
    "    fts4=group.apply(lambda x:x.mean())\n",
    "    fts4.columns=[f+\"_mean\" for f in fields]\n",
    "    fts=pd.concat([fts1,fts2,fts3,fts4],axis=1)\n",
    "    fts[\"UID\"]=fts1.index\n",
    "    print(\"=====OK======\")\n",
    "    return fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transaction表\n",
    "# 6、divice_code1、2，device1、mac1、geo_code缺失值，羊毛党明显偏高羊毛党：空值数量或者是否包含空值\n",
    "# ip1,ip1_sub,device2也有20%偏高\n",
    "#获得空值率\n",
    "tr_fields6=[\"device_code1\",\"device_code2\",\"device1\",\"mac1\",\"geo_code\",\"ip1\",\"ip1_sub\",\"device2\"]\n",
    "def get_tr_null_fts6(data,fields):\n",
    "    group=data.groupby(\"UID\")\n",
    "    fts=pd.DataFrame(group.apply(lambda x:len(x)))\n",
    "    fts.columns=[\"count\"]\n",
    "    for field in fields:\n",
    "        fts=pd.concat([fts,group[field].apply(lambda x:len(x[x.isnull()]))],axis=1)\n",
    "    fts=fts.div(fts[\"count\"],axis=0)\n",
    "    fts=fts.reset_index(drop=False)\n",
    "    fts=fts.drop([\"count\"],axis=1)\n",
    "    print(\"=====OK======\")\n",
    "    return fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n",
      "=====OK======\n"
     ]
    }
   ],
   "source": [
    "train_op_fts1,test_op_fts1=get_discrete_fts1(train_op,test_op,op_fields1)\n",
    "train_tr_fts1,test_tr_fts1=get_discrete_fts1(train_tr,test_tr,tr_fields1)\n",
    "\n",
    "train_op_fts2=get_kinds_fts2(train_op,op_fields2)\n",
    "train_tr_fts2=get_kinds_fts2(train_tr,tr_fields2)\n",
    "train_op_fts3=get_fields_as_key_fts3(train_op,op_fields3)\n",
    "train_tr_fts3=get_fields_as_key_fts3(train_tr,tr_fields3)\n",
    "train_op_fts4=get_sample_counts_fts4(train_op)\n",
    "train_tr_fts4=get_sample_counts_fts4(train_tr)\n",
    "train_tr_fts5=get_tr_money_fts5(train_tr,tr_fields5)\n",
    "train_tr_fts6=get_tr_null_fts6(train_tr,tr_fields6)\n",
    "\n",
    "test_op_fts2=get_kinds_fts2(test_op,op_fields2)\n",
    "test_tr_fts2=get_kinds_fts2(test_tr,tr_fields2)\n",
    "test_op_fts3=get_fields_as_key_fts3(test_op,op_fields3)\n",
    "test_tr_fts3=get_fields_as_key_fts3(test_tr,tr_fields3)\n",
    "test_op_fts4=get_sample_counts_fts4(test_op)\n",
    "test_tr_fts4=get_sample_counts_fts4(test_tr)\n",
    "test_tr_fts5=get_tr_money_fts5(test_tr,tr_fields5)\n",
    "test_tr_fts6=get_tr_null_fts6(test_tr,tr_fields6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fts=pd.DataFrame(pd.concat([train_op.UID,train_tr.UID],axis=0).unique())\n",
    "train_fts.columns=[\"UID\"]\n",
    "train_fts.sort_values(\"UID\",inplace=True)\n",
    "train_fts=train_fts.merge(train_op_fts1,how=\"left\",on=\"UID\")\n",
    "train_fts=train_fts.merge(train_tr_fts1,how=\"left\",on=\"UID\")\n",
    "train_fts=train_fts.merge(train_op_fts2,how=\"left\",on=\"UID\")\n",
    "train_fts=train_fts.merge(train_tr_fts2,how=\"left\",on=\"UID\")\n",
    "train_fts=train_fts.merge(train_op_fts3,how=\"left\",on=\"UID\")\n",
    "train_fts=train_fts.merge(train_tr_fts3,how=\"left\",on=\"UID\")\n",
    "train_fts=train_fts.merge(train_op_fts4,how=\"left\",on=\"UID\")\n",
    "train_fts=train_fts.merge(train_tr_fts4,how=\"left\",on=\"UID\")\n",
    "train_fts=train_fts.merge(train_tr_fts5,how=\"left\",on=\"UID\")\n",
    "#train_fts=train_fts.merge(train_tr_fts6,how=\"left\",on=\"UID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fts=pd.read_csv(r\"D:\\DA_competition\\DC\\data\\sub_sample.csv\")\n",
    "test_fts=test_fts.drop(\"Tag\",axis=1)\n",
    "test_fts=test_fts.merge(test_op_fts1,how=\"left\",on=\"UID\")\n",
    "test_fts=test_fts.merge(test_tr_fts1,how=\"left\",on=\"UID\")\n",
    "test_fts=test_fts.merge(test_op_fts2,how=\"left\",on=\"UID\")\n",
    "test_fts=test_fts.merge(test_tr_fts2,how=\"left\",on=\"UID\")\n",
    "test_fts=test_fts.merge(test_op_fts3,how=\"left\",on=\"UID\")\n",
    "test_fts=test_fts.merge(test_tr_fts3,how=\"left\",on=\"UID\")\n",
    "test_fts=test_fts.merge(test_op_fts4,how=\"left\",on=\"UID\")\n",
    "test_fts=test_fts.merge(test_tr_fts4,how=\"left\",on=\"UID\")\n",
    "test_fts=test_fts.merge(test_tr_fts5,how=\"left\",on=\"UID\")\n",
    "#test_fts=test_fts.merge(test_tr_fts6,how=\"left\",on=\"UID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UID', 'mode_00094ae2a1d62504', 'mode_072eee5c88d380df',\n",
       "       'mode_08017d2cb28c2348', 'mode_09080b31b40d57e8',\n",
       "       'mode_0e72a7851fadf00b', 'mode_12845a3fe90eb1de',\n",
       "       'mode_148dc2618a3a92a4', 'mode_17a6ee1a7b6a02b7',\n",
       "       'mode_1c341176507fbd9b',\n",
       "       ...\n",
       "       'UID_sample_counts_y', 'trans_amt_sum', 'bal_sum', 'trans_amt_max',\n",
       "       'bal_max', 'trans_amt_min', 'bal_min', 'trans_amt_mean', 'bal_mean',\n",
       "       'Tag'],\n",
       "      dtype='object', length=442)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fts.fillna(0,inplace=True)\n",
    "test_fts.fillna(0,inplace=True)\n",
    "train_fts=pd.merge(train_fts,train_tag,how=\"left\",on=\"UID\")\n",
    "train_fts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.array(train_fts.iloc[:,1:-1])\n",
    "y=np.array(train_fts.iloc[:,-1])\n",
    "X_train,X_verify,y_train,y_verify=train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "X_test=np.array(test_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "train_set=lgb.Dataset(X_train,y_train)\n",
    "params={'boosting':'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 6,\n",
    "        'num_leaves': 31,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.2,\n",
    "        'min_child_weight': 25,\n",
    "        }\n",
    "n_rounds=1000\n",
    "clf=lgb.train(params,train_set,n_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tpr_weight_funtion(y_true,y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70233644859813082"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#验证集\n",
    "y_verify_pre=clf.predict(X_verify)\n",
    "tpr_weight_funtion(y_verify,y_verify_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#测试集\n",
    "train_set=lgb.Dataset(X,y)\n",
    "clf=lgb.train(params,train_set,n_rounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_submit=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=pd.concat([test_fts[\"UID\"],pd.Series(y_submit)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.columns=[\"UID\",\"Tag\"]\n",
    "result.to_csv(r\"D:\\DA_competition\\DC\\data\\result_without_6.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_fts.to_csv(r\"D:\\DA_competition\\DC\\data\\train_fts_without_6.csv\",index=False)\n",
    "# test_fts.to_csv(r\"D:\\DA_competition\\DC\\data\\test_fts_without_6.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#减少1：线下0.71507009345794392 线上：0.06081\n",
    "#减少2：线下：0.74836448598130845 线上：0.00***9\n",
    "#减少1、6：线下：0.70992990654205612 线上：0.069519\n",
    "#减少1、6、4：线上：0.04\n",
    "#减少1、3：线下：0.66635514018691588 线上：0.01\n",
    "#减少1、4：线下：0.70712616822429908 线上：0.00***6\n",
    "#减少1、5：线下：0.70864485981308412 线上：0.070254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#one-hot的时候特征的顺序全都不对了，就很烦，训练集测试集应该并在一起进行one-hot cross-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#修改了代码bug cross-type:线下：0.71343457943925226 线上：0.05\n",
    "\n",
    "#code1,code2\n",
    "#删除了空值很多的字段，合并了可合并项，取消了fts3填充0值\n",
    "#线下：0.71285046728971957，线上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
